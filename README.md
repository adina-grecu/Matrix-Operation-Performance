
This project aims to implement the operation C = A * B * At + Bt * Bt using three different methods:

The first method utilizes the BLAS (Basic Linear Algebra Subprograms) library to perform the matrix multiplications efficiently, taking advantage of optimized functions such as dtrmm and dgemm.

The second method involves a normal matrix multiplication approach, where the multiplication is performed using nested loops. Although less optimized, this method provides a baseline for comparison.

The third method focuses on an optimized implementation that maintains the same complexity as the previous approach. By employing techniques such as register variables and reducing memory accesses, the optimized code aims to enhance performance without compromising the algorithm's complexity.

By comparing the performance and efficiency of these three methods, this project seeks to analyze the impact of using the BLAS library and optimizing the code on the overall execution time and memory usage.

## BLAS Implementation

For the BLAS method implementation, the dtrmm and dgemm functions from the library were utilized. The dtrmm function was used to handle the fact that matrix A is upper triangular.

Firstly, a copy of matrix B was made since the result of A * B * At was stored in matrix B. Thus, A and B were multiplied using dtrmm, specifying that the multiplication is done on the left (as A is the first parameter and B is the second) and that A is upper triangular and not transposed and contains non-zero values on the main diagonal.

Similarly, dtrmm was called again to multiply the previous result with A transposed. The function call is similar, except that now the multiplication is done on the right (as the first parameter is A transposed) and the result is saved in B (which now contains A * B).

Finally, the dgemm function was used to calculate Bt * Bt simultaneously with the sum Bt * Bt + A * B * At. The dgemm function calculates (op A) * (op B) + C (with alpha and beta as 1). In this case, (op A) and (op B) are B transposed, and C is A * B * At, the result stored in B. The function was called using Bcopy, and the result was stored in C.

## Unoptimized Implementation

For the unoptimized implementation, the calculations were performed using nested loops. Firstly, A * B was computed, where the inner loop's k was greater than or equal to i to avoid unnecessary multiplications with 0 generated by matrix A. Then, A * B * At was calculated using the previous result. The loops were structured similarly, with the difference that now k had to be greater than j, and matrix A was traversed column-wise instead of row-wise since A is transposed. Likewise, for Bt * Bt, the traversal was reversed, and all loops started from 0. Finally, the obtained results were added to C.

It should be noted that in this version, calloc was used to resolve errors from Valgrind. (the optimized version uses malloc)

## Optimized Implementation

For the optimized implementation, all variables were declared using the register keyword, and an intermediate sum variable was used to reduce the number of memory accesses.

## Cache Analysis

The results obtained by running Valgrind with cachegrind reflect my expectations regarding the efficiency of each method. The variant with BLAS has the fewest memory references (~250 million), followed by the optimized variant (~2.9 billion), and finally the unoptimized variant (~5.9 billion). It was expected that the BLAS variant would have the most efficient memory access since it utilizes library functions that are optimized for efficiency. The optimizations made in the code, such as using register variables, also contribute to improved memory access compared to the unoptimized variant. Interestingly, the optimized variant has slightly more misspredicts than the unoptimized variant, despite its more efficient memory access. This may be due to the use of register variables, which can lead to an increase in misspredicts. Additionally, the BLAS variant has more LLd misses and LLi misses than the other variants, suggesting that the BLAS library may use data structures that are not optimally stored in the cache.

## Comparative Performance Analysis

In the graph attached, the numbers from 1 to 5 on the horizontal axis represent the 5 tests with N equal to 400, 600, 800, 1000, and 1200. It can be observed that the BLAS variant has a linear performance, while the unoptimized and optimized variants tend to be exponential as N increases. This happens because as the code becomes more efficient, the program remains performant even for very large values. In the case of the BLAS variant, the performance is constant, explaining the linearity of the graph. In the case of the optimized variant, the performance is better than the unoptimized one, as seen in the graph by its less steep growth compared to the unoptimized variant. In the case of the unoptimized variant, the performance is the weakest, leading to an exponential decrease in performance as N increases.